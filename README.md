# ğŸš€ ai-docs-assistant-api

> ğŸ§  **GenAI (RAG)** â€¢ ğŸ **FastAPI** â€¢ ğŸ—ƒï¸ **PostgreSQL + pgvector** â€¢ ğŸ”Œ **LangChain + OpenAI** â€¢ ğŸ³ **Docker Compose** â€¢ ğŸ“¨ **Celery + Redis** â€¢ ğŸ” **API Key**

A modern, production-ready **RAG API** for querying your own documents. Upload PDFs or text, the service chunks and embeds them into **Postgres/pgvector**, then answers questions with **LLM** responses grounded in retrieved context (with citations).

---

## âœ¨ Highlights

* ğŸ—ƒï¸ **PostgreSQL + pgvector** for vector search (single source of truth, easy backups)
* ğŸ **FastAPI** with OpenAPI docs (`/docs`, `/redoc`)
* ğŸ”Œ **LangChain + OpenAI**: `text-embedding-3-small` (default) + configurable chat model
* ğŸ§© **RAG pipeline**: chunking, embeddings, ANN retrieval, citations
* ğŸ“¨ **Async ingestion**: **Celery + Redis** (sync fallback in dev)
* ğŸ” **Simple auth**: `X-API-Key` header (optional in dev)
* ğŸ³ **Docker Compose**: one file, one command

**API Docs:**

* ğŸ§­ Swagger UI â†’ `http://localhost:8080/docs`
* ğŸ“œ OpenAPI JSON â†’ `http://localhost:8080/openapi.json`

---

## ğŸ—ï¸ Architecture

```
ai-docs-assistant-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                 ğŸ® Endpoints (routers, deps)
â”‚   â”œâ”€â”€ core/                ğŸ”§ Config, logging
â”‚   â”œâ”€â”€ db/                  ğŸ—„ï¸ SQLAlchemy models, init, repos
â”‚   â”œâ”€â”€ ingestion/           ğŸ“¥ Parsers + Celery tasks/worker
â”‚   â””â”€â”€ rag/                 ğŸ§  Chunker, embeddings, retriever, pipeline, prompts
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ api/Dockerfile       ğŸ³ API image
â”‚   â””â”€â”€ worker/Dockerfile    ğŸ³ Celery worker image
â”œâ”€â”€ compose.yaml             ğŸ§© Postgres+pgvector, Redis, API, Worker
â”œâ”€â”€ pyproject.toml           ğŸ“¦ Dependencies & tooling
â””â”€â”€ README.md                ğŸ“˜ This file
```

**RAG flow (high level):**

1. `POST /v1/documents/upload` â†’ extract text (PDF/UTF-8)
2. **Ingest**: chunking â†’ embeddings â†’ store in `pgvector`
3. `POST /v1/chat/query` â†’ embed question â†’ ANN search â†’ LLM answer + **citations**

---

## ğŸ³ Quick Start (Docker Compose)

Requirements: Docker, Docker Compose, an OpenAI API key.

```bash
# 0) Configure environment
cp .env.example .env
# edit .env and set OPENAI_API_KEY

# 1) Build
docker compose build

# 2) Run (foreground or -d)
docker compose up
# or
docker compose up -d

# 3) Tail logs
docker compose logs -f api
```

Endpoints after startup:

* Health: `http://localhost:8080/v1/health`
* Swagger UI: `http://localhost:8080/docs`

Stop & clean:

```bash
docker compose down              # stop
docker compose down -v           # stop + remove volumes (reset DB)
```

> Changed the API code?
> `docker compose build api && docker compose up -d`

---

## ğŸ” Auth

By default the API expects `X-API-Key`. In dev you can leave it empty to disable auth.

```
X-API-Key: dev-key-123
```

Configure in `.env`.

---

## ğŸ“š Endpoints (v1)

### Health

* `GET /v1/health` â†’ service & DB status

### Documents

* `POST /v1/documents/upload` (multipart `file`) â†’ `{ "document_id": "<UUID>" }`
  *Automatically tries Celery ingestion; in dev falls back to sync if the worker is down.*
* `POST /v1/documents/{document_id}/ingest` â†’ force ingestion
* `GET  /v1/documents/{document_id}/status` â†’ `uploaded | processing | ready | failed`

### Retrieval (debug)

* `GET /v1/chunks/search?q=...&top_k=5` â†’ preview top-K chunks (content + similarity)

### Chat (RAG)

* `POST /v1/chat/query` â†’ `{ "answer": "...", "citations": [...] }`
  Request body:

  ```json
  {
    "query": "What is X?",
    "top_k": 6,
    "temperature": 0.0,
    "max_tokens": 400,
    "document_ids": ["...optional..."]
  }
  ```

---

## ğŸ§° Example cURL

Assume `KEY="dev-key-123"` and you have `file.pdf`.

**1) Upload a document**

```bash
curl -s -X POST http://localhost:8080/v1/documents/upload \
  -H "X-API-Key: $KEY" \
  -F "file=@./file.pdf"
# => {"document_id":"<UUID>"}
```

**2) Check status**

```bash
DOC="<UUID>"
curl -s "http://localhost:8080/v1/documents/$DOC/status" \
  -H "X-API-Key: $KEY"
```

**3) (Optional) Force ingestion**

```bash
curl -s -X POST "http://localhost:8080/v1/documents/$DOC/ingest" \
  -H "X-API-Key: $KEY"
```

**4) Ask a question (RAG)**

```bash
curl -s -X POST http://localhost:8080/v1/chat/query \
  -H "X-API-Key: $KEY" -H "Content-Type: application/json" \
  -d "{\"query\":\"Summarise key points\",\"top_k\":6}"
```

**5) Debug top-K chunks**

```bash
curl -s "http://localhost:8080/v1/chunks/search?q=topic&top_k=5" \
  -H "X-API-Key: $KEY"
```

---

## âš™ï¸ Configuration (.env)

```env
APP_ENV=dev
API_TITLE=ai-docs-assistant-api
API_VERSION=0.1.0

# OpenAI
OPENAI_API_KEY=sk-...
CHAT_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small
EMBED_DIM=1536

# Postgres + pgvector
DB_DSN=postgresql+psycopg://docs:docs@db:5432/docs

# Celery / Redis
REDIS_URL=redis://redis:6379/0
CELERY_CONCURRENCY=2

# Auth
API_KEY=dev-key-123

# Retrieval / Chunking
TOP_K=6
CHUNK_SIZE=1000
CHUNK_OVERLAP=150
```

Notes:

* DB image in `compose.yaml` is `pgvector/pgvector:pg16` (the `vector` extension is created on startup).
* An IVFFLAT `vector_cosine_ops` index is created automatically (`lists = 100`).

---

## ğŸ§  RAG Details

* **Chunking:** \~`1000` chars, overlap `150` (configurable)
* **Embeddings:** `text-embedding-3-small` (dim `1536`)
* **ANN retrieval:** k-NN with cosine distance in pgvector
* **Answering:** chat LLM (default `gpt-4o-mini`) with **citations** (snippet + `document_id`, `chunk_index`)
* **Guardrails:** prompt instructs â€œanswer only from context; otherwise say you donâ€™t knowâ€

---

## ğŸ§ª Testing (recommended)

* **pytest** â€“ unit + e2e
* **httpx** â€“ ASGI API tests
* **Fakes** for embeddings/LLM to avoid external calls in CI
* Cover: PDF/text parsing, chunking, retrieval, end-to-end upload â†’ ingest â†’ query

---

## ğŸ“Š Observability

* Structured logs (JSON-friendly) with `request_id` (ready for extension)
* (Nice-to-have) **Prometheus** metrics & **OpenTelemetry** traces (API â†’ retrieval â†’ LLM)

---

## ğŸ§­ Troubleshooting

* `HTTP 500` on `/chat/query` â†’ check `OPENAI_API_KEY` and model access
* `status = failed` after ingest â†’ the document may not contain extractable text (scanned PDFs need OCR)
* No vectors stored â†’ ensure `CREATE EXTENSION vector` ran (the app does this on startup)
* Celery worker down â†’ ingestion falls back to **synchronous** mode (slower but fine for demos)

---

## ğŸ—ºï¸ Roadmap

* ğŸ” Reranking (HF cross-encoder / LLM re-rank)
* â• Query expansion (multi-query)
* ğŸ§¾ Eval harness (hit\@K, MRR, LLM-graded faithfulness)
* ğŸ·ï¸ Multi-tenant (`tenant_id` across tables)
* ğŸš¦ Rate limiting (Redis) & query audit
* ğŸ§¹ OCR (e.g., Tesseract/TrOCR) for scanned PDFs
* ğŸ” Secret manager & PII redaction policies
* ğŸ“ˆ Prometheus/Grafana, OpenTelemetry traces

---

## ğŸ“„ License

MIT (add a `LICENSE` file if desired).
