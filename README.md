# ğŸš€ ai-docs-assistant-api

> ğŸ§  **GenAI (RAG)** â€¢ ğŸ **FastAPI** â€¢ ğŸ—ƒï¸ **PostgreSQL + pgvector** â€¢ ğŸ”Œ **LangChain + OpenAI** â€¢ ğŸ³ **Docker Compose** â€¢ ğŸ“¨ **Celery + Redis** â€¢ ğŸ” **API Key**

A production-ready **RAG API** for asking questions about your own documents. Upload PDFs or text, the service chunks and embeds them into **Postgres/pgvector**, then answers queries with **LLM** responses **grounded** in retrieved context (with citations).

---

## âœ¨ Highlights

* ğŸ—ƒï¸ **PostgreSQL + pgvector** for vector search (one source of truth, easy backups)
* ğŸ **FastAPI 0.116+** with OpenAPI docs (`/docs`, `/redoc`)
* ğŸ§± **LangChain + OpenAI**: `text-embedding-3-small` (default) + chat model (configurable)
* ğŸ§© **RAG pipeline**: chunking, embedding, ANN retrieval, citations
* ğŸ“¨ **Asynchroniczny ingest**: **Celery + Redis** (fallback sync w dev)
* ğŸ” **Proste auth**: `X-API-Key` header (opcjonalne w DEV)
* ğŸ§ª **Testy** (pytest + httpx) â€“ rekomendowane
* ğŸ³ **Docker + Compose**: jeden plik, jeden start

**API Docs:**

* ğŸ§­ Swagger UI â†’ `http://localhost:8080/docs`
* ğŸ“œ OpenAPI JSON â†’ `http://localhost:8080/openapi.json`

---

## ğŸ—ï¸ Architecture

```
ai-docs-assistant-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                 ğŸ® Endpoints (routers, deps)
â”‚   â”œâ”€â”€ core/                ğŸ”§ Config, logging
â”‚   â”œâ”€â”€ db/                  ğŸ—„ï¸ SQLAlchemy models, init, repos
â”‚   â”œâ”€â”€ ingestion/           ğŸ“¥ Parsers + Celery tasks/worker
â”‚   â””â”€â”€ rag/                 ğŸ§  Chunker, embeddings, retriever, pipeline, prompts
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ api/Dockerfile       ğŸ³ API image
â”‚   â””â”€â”€ worker/Dockerfile    ğŸ³ Celery worker image
â”œâ”€â”€ compose.yaml             ğŸ§© Postgres+pgvector, Redis, API, Worker
â”œâ”€â”€ pyproject.toml           ğŸ“¦ Dependencies & tooling
â””â”€â”€ README.md                ğŸ“˜ This file
```

**RAG flow (high level):**

1. `POST /v1/documents/upload` â†’ ekstrakcja tekstu (PDF/UTF-8)
2. **Ingest**: chunking â†’ embeddings â†’ zapis do `pgvector`
3. `POST /v1/chat/query` â†’ embed zapytania â†’ ANN search â†’ LLM answer + **citations**

---

## ğŸ³ Quick Start (Docker Compose)

Wymagania: Docker, Docker Compose, klucz do OpenAI.

```bash
# 0) Kopia env
cp .env.example .env
# Edytuj .env i ustaw OPENAI_API_KEY

# 1) Build
docker compose build

# 2) Run (foreground albo -d)
docker compose up
# lub
docker compose up -d

# 3) Logs
docker compose logs -f api
```

Endpoints po starcie:

* API (health): `http://localhost:8080/v1/health`
* Swagger UI: `http://localhost:8080/docs`

Stop & clean:

```bash
docker compose down              # stop
docker compose down -v           # stop + remove volumes (reset DB)
```

> ZmieniÅ‚eÅ› kod API?
> `docker compose build api && docker compose up -d`

---

## ğŸ” Auth (proste)

DomyÅ›lnie uÅ¼ywamy nagÅ‚Ã³wka `X-API-Key`. W DEV moÅ¼na pominÄ…Ä‡ (gdy `API_KEY` puste).
Dodaj do Å¼Ä…daÅ„:

```
X-API-Key: dev-key-123
```

(ustaw w `.env` swojÄ… wartoÅ›Ä‡)

---

## ğŸ“š Endpoints (v1)

### Health

* `GET /v1/health` â†’ status serwisu i DB

### Documents

* `POST /v1/documents/upload` (multipart `file`) â†’ `{ document_id }`
  *Automatycznie prÃ³buje uruchomiÄ‡ ingest (Celery), w DEV spadnie do trybu synchronicznego, jeÅ›li worker nie dziaÅ‚a.*
* `POST /v1/documents/{document_id}/ingest` â†’ rÄ™czne uruchomienie ingestu
* `GET  /v1/documents/{document_id}/status` â†’ `uploaded | processing | ready | failed`

### Retrieval (debug)

* `GET /v1/chunks/search?q=...&top_k=5` â†’ podglÄ…d top-K chunkÃ³w (zawartoÅ›Ä‡ + similarity)

### Chat (RAG)

* `POST /v1/chat/query` â†’ `{ answer, citations[] }`
  Body:

  ```json
  {
    "query": "What is X?",
    "top_k": 6,
    "temperature": 0.0,
    "max_tokens": 400,
    "document_ids": ["...optional..."]
  }
  ```

---

## ğŸ§° Example cURL

> ZaÅ‚Ã³Å¼my `KEY="dev-key-123"` i masz `file.pdf`.

**1) Upload dokumentu**

```bash
curl -s -X POST http://localhost:8080/v1/documents/upload \
  -H "X-API-Key: $KEY" \
  -F "file=@./file.pdf"
# => {"document_id":"<UUID>"}
```

**2) SprawdÅº status**

```bash
DOC="<UUID>"
curl -s "http://localhost:8080/v1/documents/$DOC/status" \
  -H "X-API-Key: $KEY"
```

**3) (Opcjonalnie) WymuÅ› ingest**

```bash
curl -s -X POST "http://localhost:8080/v1/documents/$DOC/ingest" \
  -H "X-API-Key: $KEY"
```

**4) Zapytanie RAG**

```bash
curl -s -X POST http://localhost:8080/v1/chat/query \
  -H "X-API-Key: $KEY" -H "Content-Type: application/json" \
  -d "{\"query\":\"Summarise key points\",\"top_k\":6}"
```

**5) Debug top-K chunkÃ³w**

```bash
curl -s "http://localhost:8080/v1/chunks/search?q=topic&top_k=5" \
  -H "X-API-Key: $KEY"
```

---

## âš™ï¸ Configuration (.env)

```env
APP_ENV=dev
API_TITLE=ai-docs-assistant-api
API_VERSION=0.1.0

# OpenAI
OPENAI_API_KEY=sk-...
CHAT_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small
EMBED_DIM=1536

# Postgres + pgvector
DB_DSN=postgresql+psycopg://docs:docs@db:5432/docs

# Celery / Redis
REDIS_URL=redis://redis:6379/0
CELERY_CONCURRENCY=2

# Auth
API_KEY=dev-key-123

# Retrieval / Chunking
TOP_K=6
CHUNK_SIZE=1000
CHUNK_OVERLAP=150
```

**Uwaga:**

* Obraz DB w `compose.yaml` to `pgvector/pgvector:pg16` (rozszerzenie `vector` tworzone na starcie).
* Indeks IVFFLAT `vector_cosine_ops` jest zakÅ‚adany automatycznie (z `lists = 100`).

---

## ğŸ§  RAG details

* **Chunking:** ok. `1000` znakÃ³w, overlap `150` (konfigurowalne)
* **Embeddings:** `text-embedding-3-small` (dim `1536`)
* **ANN retrieval:** kNN w pgvector (`cosine`)
* **Answering:** chat LLM (domyÅ›lnie `gpt-4o-mini`) z **citations** (snippet + `document_id`, `chunk_index`)
* **Guardrails:** prompt wymusza â€odpowiadaj tylko z kontekstu, w razie braku â€“ powiedz, Å¼e nie wieszâ€

---

## ğŸ§ª Testing

Rekomendowane:

* **pytest** â€“ unit + e2e
* **httpx** â€“ testy API (ASGI)
* **Fakes** dla embedding/LLM (Å¼eby testy nie trafiaÅ‚y do chmury)
* **Migracje**: inicjalizacja schematu przy starcie testÃ³w (moÅ¼esz uÅ¼yÄ‡ osobnej instancji Postgresa lub tymczasowego vol ume)

Przykryj:

* parsowanie PDF/tekstÃ³w
* chunking (rozmiar/overlap)
* retrieval (zapytania â†’ top-K)
* Å›cieÅ¼ka upload â†’ ingest â†’ query (e2e)

---

## ğŸ“Š Observability

* **Structured logs** (JSON-friendly) z `request_id` (do rozbudowy)
* (Nice-to-have) **Prometheus** metrics i **OpenTelemetry** traces (API â†’ retrieval â†’ LLM)

---

## ğŸ§­ Troubleshooting

* `HTTP 500` przy query â†’ sprawdÅº `OPENAI_API_KEY` i dostÄ™p do modeli
* `failed` status ingestu â†’ upewnij siÄ™, Å¼e dokument ma tekst (skanowane PDFy wymagajÄ… OCR â€“ patrz â€Roadmapâ€)
* Brak wektorÃ³w â†’ upewnij siÄ™, Å¼e `CREATE EXTENSION vector` zostaÅ‚o wykonane (aplikacja robi to przy starcie)
* Worker nie dziaÅ‚a â†’ ingest wykona siÄ™ **synchronicznie** w procesie API (wolniej, ale nie blokuje demo)

---

## ğŸ—ºï¸ Roadmap (nice-to-have)

* ğŸ” **Reranking** (HF cross-encoder / LLM re-rank)
* â• **Query expansion** (multi-query)
* ğŸ§¾ **Eval harness** (hit\@K, MRR, LLM-graded faithfulness)
* ğŸ·ï¸ **Multi-tenant** (`tenant_id` w kaÅ¼dej tabeli)
* ğŸš¦ **Rate limiting** (Redis) i audyt zapytaÅ„
* ğŸ§¹ **OCR** (np. Tesseract/TrOCR) dla skanÃ³w PDF
* ğŸ” **Secret manager** i lepsze polityki PII
* ğŸ“ˆ **Prometheus/Grafana**, **OTel** traces

---

## âš ï¸ Notes

* Generowanie embeddingÃ³w i odpowiedzi korzysta z zewnÄ™trznych modeli â€“ pamiÄ™taj o **kosztach** i limitach.
* Ten projekt przechowuje tylko **tekst** i **embeddingi**; treÅ›ci wraÅ¼liwe loguj ostroÅ¼nie.

---

## ğŸ“„ License

MIT (jeÅ›li chcesz, dodam plik `LICENSE`).

---

Chcesz, Å¼ebym od razu podmieniÅ‚a plik `README.md` w Twojej paczce i dorzuciÅ‚a go do ZIPa pod nowÄ… nazwÄ… projektu **ai-docs-assistant-api**?
